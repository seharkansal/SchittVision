{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/home/sehar/schittVision/schitts_creek_combined_dialogues.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36011\n",
      "Character    0\n",
      "Dialogue     0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [Character, Dialogue]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])  # Total number of rows\n",
    "print(df.isnull().sum())  # Count NaN values\n",
    "print(df[df['Dialogue'].str.strip() == ''])  # Check for empty strings\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Dialogue'])  # Drop rows where Dialogue is NaN\n",
    "df = df[df['Dialogue'].str.strip() != '']  # Drop rows with empty Dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36011\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])  # Confirm the number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Character                                           Dialogue\n",
      "0   unknown  Episode transcripts for the TV show \"Schitt's ...\n",
      "1   unknown                                      (Birds chirp)\n",
      "2   unknown                                   (Footsteps echo)\n",
      "3   unknown                                   (Footstep clomp)\n",
      "4   unknown                                (Doors creaks open)\n"
     ]
    }
   ],
   "source": [
    "# View the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character    0\n",
      "Dialogue     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of       Character                                           Dialogue\n",
       "0       unknown  Episode transcripts for the TV show \"Schitt's ...\n",
       "1       unknown                                      (Birds chirp)\n",
       "2       unknown                                   (Footsteps echo)\n",
       "3       unknown                                   (Footstep clomp)\n",
       "4       unknown                                (Doors creaks open)\n",
       "...         ...                                                ...\n",
       "36006   unknown                                          Love you.\n",
       "36007   unknown                                      Johnny: Wait!\n",
       "36008   unknown                                      Stop the car!\n",
       "36009   unknown                                        What is it?\n",
       "36010   unknown                         Just wanted one last look.\n",
       "\n",
       "[36011 rows x 2 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing dialogues\n",
    "df.dropna(subset=['Dialogue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of       Character                                           Dialogue\n",
       "0       unknown  Episode transcripts for the TV show \"Schitt's ...\n",
       "1       unknown                                      (Birds chirp)\n",
       "2       unknown                                   (Footsteps echo)\n",
       "3       unknown                                   (Footstep clomp)\n",
       "4       unknown                                (Doors creaks open)\n",
       "...         ...                                                ...\n",
       "36006   unknown                                          Love you.\n",
       "36007   unknown                                      Johnny: Wait!\n",
       "36008   unknown                                      Stop the car!\n",
       "36009   unknown                                        What is it?\n",
       "36010   unknown                         Just wanted one last look.\n",
       "\n",
       "[36011 rows x 2 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to schitts_creek_dialogues_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to clean the dialogue\n",
    "def clean_dialogue(dialogue):\n",
    "    # Remove unwanted characters or excessive spaces\n",
    "    dialogue = re.sub(r'[^a-zA-Z0-9\\s.,!?;\\'\\\"-]', '', dialogue)\n",
    "    dialogue = re.sub(r'\\s+', ' ', dialogue)  # Replace multiple spaces with single space\n",
    "    return dialogue.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df['Dialogue'] = df['Dialogue'].apply(clean_dialogue)\n",
    "\n",
    "# Drop empty or very short dialogues\n",
    "df = df[df['Dialogue'].apply(lambda x: len(x.split()) > 3)]  # Keeps dialogues with more than 3 words\n",
    "\n",
    "# Save the cleaned CSV\n",
    "df.to_csv('schitts_creek_dialogues_cleaned.csv', index=False)\n",
    "print(\"Cleaned data saved to schitts_creek_dialogues_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of            Character                                           Dialogue\n",
       "0            unknown  Episode transcripts for the TV show \"Schitt's ...\n",
       "7      Revenue agent  Missus Rose! There are people here from the go...\n",
       "10     Revenue agent  John, I've been stripped of every morsel of pl...\n",
       "11            Johnny             Well, how do you think I feel, Moira?!\n",
       "12            Johnny                    Eli was family, for God's sake!\n",
       "...              ...                                                ...\n",
       "35999        unknown                                I'm so proud of us.\n",
       "36003        unknown                                      - I love you!\n",
       "36004        unknown                                  - I love you too.\n",
       "36005        unknown                                 Moira We love you!\n",
       "36010        unknown                         Just wanted one last look.\n",
       "\n",
       "[24690 rows x 2 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('schitts_creek_dialogues_cleaned.csv')\n",
    "\n",
    "# # Define the characters you want to keep\n",
    "# wanted_characters = ['Moira', 'Johnny', 'David', 'Alexis']\n",
    "\n",
    "# # Filter the dataset to include only the wanted characters\n",
    "# df_filtered = df[df['Character'].isin(wanted_characters)]\n",
    "\n",
    "# # Save the filtered dataset\n",
    "# df_filtered.to_csv('schitts_creek_dialogues_main_characters.csv', index=False)\n",
    "# print(\"Filtered data saved to schitts_creek_dialogues_main_characters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sehar/schittVision/.venv/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24690, 128])\n",
      "Tokenized data saved to tokenized_data.pt\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('schitts_creek_dialogues_cleaned.csv')\n",
    "\n",
    "# Initialize a tokenizer (e.g., BERT tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dialogues\n",
    "tokens = tokenizer(\n",
    "    df['Dialogue'].tolist(),\n",
    "    padding=True,  # Add padding to make sequences the same length\n",
    "    truncation=True,  # Truncate sequences longer than the model's max length\n",
    "    max_length=128,  # Define max sequence length\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Inspect the tokenized output\n",
    "print(tokens['input_ids'].shape)  # Shape: (num_samples, max_length)\n",
    "\n",
    "# Save tokenized data if needed\n",
    "torch.save(tokens, 'tokenized_data.pt')\n",
    "print(\"Tokenized data saved to tokenized_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2792, 24051,  ...,     0,     0,     0],\n",
      "        [  101,  3335,  2271,  ...,     0,     0,     0],\n",
      "        [  101,  2198,  1010,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1011,  1045,  ...,     0,     0,     0],\n",
      "        [  101, 25175,  2527,  ...,     0,     0,     0],\n",
      "        [  101,  2074,  2359,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DialogueDatasetMLM(Dataset):\n",
    "    def __init__(self, dialogues, tokenizer, max_length=128, mask_prob=0.15):\n",
    "        self.dialogues = dialogues\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #dialogue = self.dialogues[idx]\n",
    "        encoding = tokens\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Apply masking\n",
    "        input_ids, labels = self._mask_tokens(input_ids)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def _mask_tokens(self, input_ids):\n",
    "        labels = input_ids.clone()  # Clone input_ids for label generation\n",
    "\n",
    "        # Create a probability matrix for masking\n",
    "        probability_matrix = torch.full(input_ids.shape, self.mask_prob, device=input_ids.device)\n",
    "\n",
    "        # Handle special tokens mask for each sequence in the batch\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(seq.tolist(), already_has_special_tokens=True)\n",
    "            for seq in input_ids\n",
    "        ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool, device=input_ids.device)\n",
    "\n",
    "        # Apply the mask to the probability matrix\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)  # Do not mask special tokens\n",
    "\n",
    "        # Select tokens to mask\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # Ignore unmasked tokens in the loss calculation\n",
    "\n",
    "        # Replace 80% of masked tokens with [MASK]\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8, device=input_ids.device)).bool() & masked_indices\n",
    "        input_ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # Replace 10% of masked tokens with random tokens\n",
    "        indices_random = (\n",
    "            torch.bernoulli(torch.full(labels.shape, 0.5, device=input_ids.device)).bool()\n",
    "            & masked_indices\n",
    "            & ~indices_replaced\n",
    "        )\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long, device=input_ids.device)\n",
    "        input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The remaining 10% of masked tokens are left unchanged\n",
    "\n",
    "        return input_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sehar/schittVision/.venv/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should be > 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return your GPU name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101,  2792, 24051,  ...,     0,     0,     0],\n",
      "        [  101,  3335,  2271,  ...,     0,     0,     0],\n",
      "        [  101,  2198,  1010,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1011,  1045,  ...,     0,     0,     0],\n",
      "        [  101, 25175,  2527,  ...,     0,     0,     0],\n",
      "        [  101,  2074, 24958,  ...,     0,     0,     0]])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "Labels: tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2359,  ..., -100, -100, -100]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/772 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load dataset with MLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "dialogues = list(df['Dialogue'])  # Assuming df['Dialogue'] contains the dialogues\n",
    "mlm_dataset = DialogueDatasetMLM(dialogues, tokenizer)\n",
    "\n",
    "# Test a single sample\n",
    "sample = mlm_dataset[0]\n",
    "print(\"Input IDs:\", sample[\"input_ids\"])\n",
    "print(\"Attention Mask:\", sample[\"attention_mask\"])\n",
    "print(\"Labels:\", sample[\"labels\"])\n",
    "\n",
    "mlm_loader = DataLoader(mlm_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define MLM Model (same transformer as before)\n",
    "# Define the Custom Transformer Encoder Model\n",
    "class TransformerEncoderMLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=256, num_layers=4, num_heads=4, max_length=128):\n",
    "        super(TransformerEncoderMLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, hidden_size))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
    "        encoded = self.encoder(embeddings, src_key_padding_mask=~attention_mask.bool())\n",
    "        logits = self.fc(encoded)\n",
    "        return logits\n",
    "\n",
    "# Initialize Model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = TransformerEncoderMLM(vocab_size=vocab_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training Setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(mlm_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        print(\"Before Forward Pass\")\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        print(\"After Forward Pass\")\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(mlm_loader)}\")\n",
    "\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'schitts_creek_pretrained.pth')\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b031f88d9e7d998b303763756b59f5e279d46025b2b69c22eea4c0556414731"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
